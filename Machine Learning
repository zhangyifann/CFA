Machine Learning 机器学习
Y：target variable；X:feature;Hyperparametere:超参数是由研究员设定的参数
Type of Machine Learning：1.supervised learning；2.unsupervised learning；3.Neural networks，deep learning，reinforcement learning
supervised learning：labeled data（有标签的数据）有监督的机器学习分为：regression（连续性数据） and classification（离散型数据）
Unsupervised learning： not given labeled data（没有标签的数据）发现数据结构，无监督的机器学习分为：Dimension reduction（降维）：减少特征的数量降低模型复杂度；Cluster（聚类）：对变量进行分组
Neutral network，Deep learning，reinforcement learning：能够处理非线性模型
data set:样本内数据 training sample 样本外数据：validation sample（用于检测完善模型），test sample（用于预测检验）
Overfitting & Underfitting，Overfitting（特征过多，模型过于复杂，对样本内的数据拟合很好，包含了噪音和一些虚假关系，对样本外的数据预测差），Underfitting（欠拟合），特征较少，模型过于简单，对样本内的数据拟合较差，
  没有准确捕捉到数据之间的关系。Bias error：Underfitting high in-sample error；variance error：Response to new data，Overfitting，high out-of sample error
  linear function：sample model，bias error； non-linear function：complex model，variance error
two methods to prevent overfitting:overfitting penalty(限制模型特征数）；cross-validation（交叉检验）k-fold cross validation：将一个样本划分为k个子集，循环将其中k-1个子集作为
    training sample，将第k个子集作为validation sample，重复k次。
Supervised learning：1.penalized regression；2.support vector machine（SVM)；3.K nearest neighbor（KNN）；4.classification andregression tree；5.ensemble and Rondom forest
SVM：最大化最短分割距离，soft margin classification：添加错误分类的惩罚项，平衡分类错误和最大分割距离；non-linear SVM，降低了模型的分类错误但是提高了模型的复杂度。
KNN：两个关键点：如何定义类似，如何设定K，K值过小容易造成分类错误，K值过大稀释了nearest neighbor的概念
CART：分类回归树；classification tree：分类，离散变量，regression tree：回归，连续变量，init nodes，decision nodes，terminal nodes
对于每个节点尽可能的扩大分类差异，降低分类误差，不断分类，减低组内差异，知道进一步分类不能降低误差时终止分类，对于分类树使用绝大多数的方法决定类型，对于回归树使用加权平均的方法进行回归值的计算。
CART相对于其他模型的优势在于提供了一个视觉的解释。
Ensemble learning：多个模型组合预测，两种方法：1.相同数据不同的模型；2.相同的模型不同的训练集数据；对于结果进行majority-vote classification，或者average for a regression
random forest：使用bagging method 对大量的决策树进行训练，然后采用majority-vote方法，提高准确性减低噪音和过度拟合，但是不足在于没办法对每个树进行观察，形成了黑箱的效果。

Unsupervised learning：无监督的机器学习

    

